{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "gpuClass": "standard"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/thomasshin/NLP_Study/blob/main/Andrej_Karpathy_Neural_Network_Zero_to_Hero/Exercises_from_lecture_2.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lURHJlAKFl3e"
      },
      "outputs": [],
      "source": [
        "from google.colab import drive\n",
        "import torch\n",
        "import torch.nn.functional as F\n",
        "import matplotlib.pyplot as plt\n",
        "import pandas as pd\n",
        "import numpy as np"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "po45qi2HGE3v",
        "outputId": "b757838f-ebd3-4955-b189-5b9c1876475f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!ls \"/content/drive/My Drive/makemore-master\""
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DkcIm27XGG2k",
        "outputId": "1db086b2-12d8-4ae0-f243-a9b164c63883"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "LICENSE  makemore.py  names.txt  README.md\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!cp \"/content/drive/My Drive/makemore-master/names.txt\" \"names.txt\""
      ],
      "metadata": {
        "id": "qN62-37JGIi9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!ls"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rm7ol_xgGM9s",
        "outputId": "5f0dbdbc-2225-4636-90ef-f127cf90a992"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "drive  names.txt  sample_data\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "words = open('names.txt', 'r').read().splitlines()"
      ],
      "metadata": {
        "id": "P5jYqwioGOpS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "chars = list(sorted(set(''.join(words))))\n",
        "stoi = {s:i+1 for i, s in enumerate(chars)}\n",
        "stoi['.'] = 0\n",
        "itos = {i:s for s, i in stoi.items()}"
      ],
      "metadata": {
        "id": "T6bbxygAGhQX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **E01**\n",
        "train a trigram language model, i.e. take two characters as an input to predict the 3rd one. Feel free to use either counting or a neural net. Evaluate the loss; Did it improve over a bigram model?"
      ],
      "metadata": {
        "id": "OJlCY1sV9cv0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#create the training set of trigrams (x, y)\n",
        "xs, ys = [], []\n",
        "\n",
        "for w in words:\n",
        "  chs = ['.'] + list(w) + ['.']\n",
        "  for ch1, ch2, ch3 in zip(chs, chs[1:], chs[2:]):\n",
        "    ix1 = stoi[ch1]\n",
        "    ix2 = stoi[ch2]\n",
        "    ix3 = stoi[ch3]\n",
        "    xs.append([ix1,ix2])\n",
        "    ys.append(ix3)\n",
        "\n",
        "xs = torch.tensor(xs)\n",
        "ys = torch.tensor(ys)"
      ],
      "metadata": {
        "id": "7YuN8ho3F_uE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "xs.shape,ys"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HQ8I6Pz9HA8O",
        "outputId": "b60dfe11-8416-4b23-de0a-dabbe7c836c6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(torch.Size([196113, 2]), tensor([13, 13,  1,  ..., 26, 24,  0]))"
            ]
          },
          "metadata": {},
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch.nn.functional as F\n",
        "g = torch.Generator().manual_seed(2147483647)\n",
        "W = torch.randn((27*2,27), generator=g, requires_grad=True)"
      ],
      "metadata": {
        "id": "QrxuBZnHH6L_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# gradient descent\n",
        "for i in range(100):\n",
        "  # forward pass\n",
        "  xenc = F.one_hot(xs, num_classes=27).float() # input to the neural network: one-hot encoding (has to be float)\n",
        "  xenc_flat = xenc.view(196113,-1)\n",
        "  logits = xenc_flat @ W # predict log-counts\n",
        "  counts = logits.exp() #counts, equivalent to N\n",
        "  probs = counts / counts.sum(dim = 1, keepdims = True) # probabilities for the next character\n",
        "  num = xenc_flat.size()[0]\n",
        "  loss = -probs[torch.arange(num), ys].log().mean() + 0.01*(W**2).mean() # model smoothing\n",
        "\n",
        "  # backward pass\n",
        "  W.grad = None # set to zero the gradient\n",
        "  loss.backward()\n",
        "\n",
        "  # update\n",
        "  W.data += -20 * W.grad\n",
        "print(loss.item())"
      ],
      "metadata": {
        "id": "qK5j7HsAIoKo",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "dd12afed-c2a2-4a82-a0d6-ff2af9b6c427"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2.3165528774261475\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "loss of trigram model with 100 epochs: 2.3165528774261475\n",
        "\n",
        "loss of bigram model with 100 epochs: 2.522418260574341\n",
        "\n",
        "so trigram model has improved over bigram model"
      ],
      "metadata": {
        "id": "Rehe6JAiRvKL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# sample from the neural net model\n",
        "g = torch.Generator().manual_seed(2147483647)\n",
        "\n",
        "for k in range(5):\n",
        "\n",
        "  out = []\n",
        "  ix0 = 0\n",
        "  ix1 = 0\n",
        "  while True:\n",
        "\n",
        "    #---------\n",
        "    # Before:\n",
        "    # p = p[ix]\n",
        "    #---------\n",
        "    # NOW:\n",
        "    xenc = F.one_hot(torch.tensor([ix0, ix1]), num_classes=27).float()\n",
        "    xenc_flat = xenc.view(-1,54)\n",
        "    logits = xenc_flat @ W\n",
        "    counts = logits.exp()\n",
        "    p = counts / counts.sum(dim=1, keepdims=True)\n",
        "    #---------\n",
        "\n",
        "    ix0 = torch.multinomial(p, num_samples=1, replacement=True, generator=g).item()\n",
        "    out.append(itos[ix0])\n",
        "    if ix0 == 0:\n",
        "      break\n",
        "  print(''.join(out))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OisQJq8v66dD",
        "outputId": "cce1f8a0-7628-4339-b57d-f99b8332b684"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "npo.\n",
            "auw.\n",
            "mnhjynrozkgq.\n",
            "drcjqadraa.\n",
            "anakxmrzapoa.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **E02**\n",
        "split up the dataset randomly into 80% train set, 10% dev set, 10% test set. Train the bigram and trigram models only on the training set. Evaluate them on dev and test splits. What can you see?"
      ],
      "metadata": {
        "id": "YPEga2T99hXi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import random\n",
        "random.seed(1)\n",
        "random.shuffle(words)"
      ],
      "metadata": {
        "id": "vlY2hfHn8WtW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#bigram\n",
        "xs, ys = [], []\n",
        "\n",
        "for w in words:\n",
        "  chs = ['.'] + list(w) + ['.']\n",
        "  for ch1, ch2 in zip(chs, chs[1:]):\n",
        "    ix1 = stoi[ch1]\n",
        "    ix2 = stoi[ch2]\n",
        "    xs.append(ix1)\n",
        "    ys.append(ix2)\n",
        "\n",
        "xs = torch.tensor(xs)\n",
        "ys = torch.tensor(ys)"
      ],
      "metadata": {
        "id": "b5Qt2rI0bG62"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X_train, y_train = xs[: int(len(xs)*0.8)], ys[: int(len(xs)*0.8)]\n",
        "X_dev, y_dev = xs[int(len(xs)*0.8) : int(len(xs)*0.9)], ys[int(len(xs)*0.8) : int(len(xs)*0.9)]\n",
        "X_test, y_test = xs[int(len(xs)*0.9) :], ys[int(len(xs)*0.9) :]"
      ],
      "metadata": {
        "id": "-9MhGtB8g2Hy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "g = torch.Generator().manual_seed(2147483647)\n",
        "W_b = torch.randn((27,27), generator = g, requires_grad = True)"
      ],
      "metadata": {
        "id": "9Ehjyu3pbwwb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# gradient descent\n",
        "for i in range(100):\n",
        "  # forward pass\n",
        "  xenc = F.one_hot(X_train, num_classes=27).float()\n",
        "  logits = xenc @ W_b\n",
        "  counts = logits.exp()\n",
        "  probs = counts / counts.sum(dim=1, keepdims=True)\n",
        "  num = xenc.shape[0]\n",
        "  loss = -probs[torch.arange(num), y_train].log().mean() + 0.01*(W_b**2).mean() # model smoothing with regularization\n",
        "\n",
        "  # backward pass\n",
        "  W_b.grad = None # set to zero the gradient\n",
        "  loss.backward()\n",
        "\n",
        "  # update\n",
        "  W_b.data += -20 * W_b.grad\n",
        "print(\"train loss for bigram :\",loss.item())"
      ],
      "metadata": {
        "id": "2qaJ85KSb_OQ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9b70ffe5-0707-433d-c402-51a7bbbfa393"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "train loss for bigram : 2.522306442260742\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "xdevenc = F.one_hot(X_dev, num_classes=27).float()\n",
        "logits = xdevenc @ W_b\n",
        "counts = logits.exp()\n",
        "probs = counts / counts.sum(dim=1, keepdims=True)\n",
        "num = xdevenc.shape[0]\n",
        "loss = -probs[torch.arange(num), y_dev].log().mean() + 0.01*(W_b**2).mean() # model smoothing with regularization\n",
        "print(\"dev loss for bigram :\", loss.item())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GVSiDu_4eiT5",
        "outputId": "e0c99948-30df-4826-a01d-c55f55c49e1b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "dev loss for bigram : 2.513911724090576\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "xtestenc = F.one_hot(X_test, num_classes=27).float()\n",
        "logits = xtestenc @ W_b\n",
        "counts = logits.exp()\n",
        "probs = counts / counts.sum(dim=1, keepdims=True)\n",
        "num = xtestenc.shape[0]\n",
        "loss = -probs[torch.arange(num), y_test].log().mean() + 0.01*(W_b**2).mean() # model smoothing with regularization\n",
        "print(\"test loss for bigram :\",loss.item())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "O25NtiOKu-JH",
        "outputId": "3c553d80-9e7b-4429-ee19-9d7860918650"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "test loss for bigram : 2.5331950187683105\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#trigram\n",
        "xs, ys = [], []\n",
        "\n",
        "for w in words:\n",
        "  chs = ['.'] + list(w) + ['.']\n",
        "  for ch1, ch2, ch3 in zip(chs, chs[1:], chs[2:]):\n",
        "    ix1 = stoi[ch1]\n",
        "    ix2 = stoi[ch2]\n",
        "    ix3 = stoi[ch3]\n",
        "    xs.append([ix1, ix2])\n",
        "    ys.append(ix3)\n",
        "\n",
        "xs = torch.tensor(xs)\n",
        "ys = torch.tensor(ys)"
      ],
      "metadata": {
        "id": "QjIjV3XzveHK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X_train, y_train = xs[: int(len(xs)*0.8)], ys[: int(len(xs)*0.8)]\n",
        "X_dev, y_dev = xs[int(len(xs)*0.8) : int(len(xs)*0.9)], ys[int(len(xs)*0.8) : int(len(xs)*0.9)]\n",
        "X_test, y_test = xs[int(len(xs)*0.9) :], ys[int(len(xs)*0.9) :]"
      ],
      "metadata": {
        "id": "-jHlNdCevm3s"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "g = torch.Generator().manual_seed(2147483647)\n",
        "W_t = torch.randn((27*2,27), generator = g, requires_grad = True)"
      ],
      "metadata": {
        "id": "nkxKz8s1vqjC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# gradient descent\n",
        "for i in range(100):\n",
        "  # forward pass\n",
        "  xenc = F.one_hot(X_train, num_classes=27).float()\n",
        "  xenc_flat = xenc.view(-1, 54)\n",
        "  logits = xenc_flat @ W_t\n",
        "  counts = logits.exp()\n",
        "  probs = counts / counts.sum(dim=1, keepdims=True)\n",
        "  num = xenc_flat.shape[0]\n",
        "  loss = -probs[torch.arange(num), y_train].log().mean() + 0.01*(W_t**2).mean() # model smoothing with regularization\n",
        "\n",
        "  # backward pass\n",
        "  W_t.grad = None # set to zero the gradient\n",
        "  loss.backward()\n",
        "\n",
        "  # update\n",
        "  W_t.data += -20 * W_t.grad\n",
        "print(\"train loss for trigram :\", loss.item())"
      ],
      "metadata": {
        "id": "8tr8tiuWvq7c",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "39c147d2-d823-4246-9cce-cf938caf3d87"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "train loss for trigram : 2.3158535957336426\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "xdevenc = F.one_hot(X_dev, num_classes=27).float()\n",
        "xdevenc_flat = xdevenc.view(-1, 54)\n",
        "logits = xdevenc_flat @ W_t\n",
        "counts = logits.exp()\n",
        "probs = counts / counts.sum(dim=1, keepdims=True)\n",
        "num = xdevenc.shape[0]\n",
        "loss = -probs[torch.arange(num), y_dev].log().mean() + 0.01*(W_t**2).mean() # model smoothing with regularization\n",
        "print(\"dev loss for trigram :\", loss.item())\n",
        "\n",
        "xtestenc = F.one_hot(X_test, num_classes=27).float()\n",
        "xtestenc_flat = xtestenc.view(-1, 54)\n",
        "logits = xtestenc_flat @ W_t\n",
        "counts = logits.exp()\n",
        "probs = counts / counts.sum(dim=1, keepdims=True)\n",
        "num = xtestenc.shape[0]\n",
        "loss = -probs[torch.arange(num), y_test].log().mean() + 0.01*(W_t**2).mean() # model smoothing with regularization\n",
        "print(\"test loss for trigram :\",loss.item())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FN5DxdHovtDZ",
        "outputId": "97365cec-102a-42c7-a99b-832152472e80"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "dev loss for trigram : 2.3124120235443115\n",
            "test loss for trigram : 2.329163074493408\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "train/dev/test losses for trigram are all lower than bigram."
      ],
      "metadata": {
        "id": "lA5r2Y9ASQlQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **E03**\n",
        "use the dev set to tune the strength of smoothing (or regularization) for the trigram model - i.e. try many possibilities and see which one works best based on the dev set loss. What patterns can you see in the train and dev set loss as you tune this strength? Take the best setting of the smoothing and evaluate on the test set once and at the end. How good of a loss do you achieve?"
      ],
      "metadata": {
        "id": "SuD1ARZw1oeB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "reg_param = torch.linspace(0.001, 1, 10)\n",
        "xdevenc = F.one_hot(X_dev, num_classes=27).float()\n",
        "xdevenc_flat = xdevenc.view(-1, 54)\n",
        "logits = xdevenc_flat @ W_t\n",
        "counts = logits.exp()\n",
        "probs = counts / counts.sum(dim=1, keepdims=True)\n",
        "num = xdevenc.shape[0]\n",
        "\n",
        "for param in reg_param:\n",
        "  loss = -probs[torch.arange(num), y_dev].log().mean() + param*(W_t**2).mean() # model smoothing with regularization\n",
        "  print(\"dev loss for trigram (param : {:10.3f}):\".format(param), loss.item()) # 0.001 works the best"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kUbuUyP01jNs",
        "outputId": "df7ad729-55f3-4c1f-8c9f-2f63583528f5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "dev loss for trigram (param :      0.001): 2.305593967437744\n",
            "dev loss for trigram (param :      0.112): 2.3896830081939697\n",
            "dev loss for trigram (param :      0.223): 2.4737720489501953\n",
            "dev loss for trigram (param :      0.334): 2.557861089706421\n",
            "dev loss for trigram (param :      0.445): 2.6419501304626465\n",
            "dev loss for trigram (param :      0.556): 2.726039171218872\n",
            "dev loss for trigram (param :      0.667): 2.8101282119750977\n",
            "dev loss for trigram (param :      0.778): 2.8942172527313232\n",
            "dev loss for trigram (param :      0.889): 2.978306293487549\n",
            "dev loss for trigram (param :      1.000): 3.0623950958251953\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "xenc = F.one_hot(X_train, num_classes=27).float()\n",
        "xenc_flat = xenc.view(-1, 54)\n",
        "logits = xenc_flat @ W_t\n",
        "counts = logits.exp()\n",
        "probs = counts / counts.sum(dim=1, keepdims=True)\n",
        "num = xenc_flat.shape[0]\n",
        "\n",
        "for param in reg_param:\n",
        "  loss = -probs[torch.arange(num), y_train].log().mean() + param*(W_t**2).mean() # model smoothing with regularization\n",
        "  print(\"train loss for trigram (param : {:10.3f}):\".format(param), loss.item()) # 0.001 works the best"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kcpsyOWO5OC8",
        "outputId": "200c4d22-cddf-4398-9808-2598a4241216"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "train loss for trigram (param :      0.001): 2.3082919120788574\n",
            "train loss for trigram (param :      0.112): 2.392380952835083\n",
            "train loss for trigram (param :      0.223): 2.4764699935913086\n",
            "train loss for trigram (param :      0.334): 2.560559034347534\n",
            "train loss for trigram (param :      0.445): 2.6446480751037598\n",
            "train loss for trigram (param :      0.556): 2.7287371158599854\n",
            "train loss for trigram (param :      0.667): 2.812826156616211\n",
            "train loss for trigram (param :      0.778): 2.8969151973724365\n",
            "train loss for trigram (param :      0.889): 2.981004238128662\n",
            "train loss for trigram (param :      1.000): 3.0650930404663086\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#using reg_param of 0.001 for test loss\n",
        "xtestenc = F.one_hot(X_test, num_classes=27).float()\n",
        "xtestenc_flat = xtestenc.view(-1, 54)\n",
        "logits = xtestenc_flat @ W_t\n",
        "counts = logits.exp()\n",
        "probs = counts / counts.sum(dim=1, keepdims=True)\n",
        "num = xtestenc_flat.shape[0]\n",
        "loss = -probs[torch.arange(num), y_test].log().mean() + 0.001*(W_t**2).mean() # model smoothing with regularization\n",
        "print(\"test loss for trigram :\", loss.item()) # test loss has improved"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Z9fQKL3y4p8h",
        "outputId": "1b8c1122-1658-4f1c-c720-e20d2bc5f54f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "test loss for trigram : 2.322345018386841\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Tuning the strength of smoothing achieved test loss of 2.322345018386841, which is slightly lower than the original\n",
        "\n",
        "test loss for original trigram : 2.329163074493408"
      ],
      "metadata": {
        "id": "oXqKU1JQVPnF"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **E04**\n",
        "we saw that our 1-hot vectors merely select a row of W, so producing these vectors explicitly feels wasteful. Can you delete our use of F.one_hot in favor of simply indexing into rows of W?"
      ],
      "metadata": {
        "id": "SLSrCUt456vS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "W_0 = torch.randn((54,27), requires_grad = True, generator = g)"
      ],
      "metadata": {
        "id": "RjOvCZ-d6CTK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for i in range(100):\n",
        "  W_1, W_2 = W[:27], W[27:]\n",
        "  logits = W_1[X_train[:, 0]] + W_2[X_train[:, 1]]\n",
        "  counts = logits.exp()\n",
        "  probs = counts / counts.sum(dim=1, keepdims=True)\n",
        "  loss = -probs[torch.arange(X_train.shape[0]), y_train].log().mean() + 0.001*(W_0**2).mean()\n",
        "  W_0.grad = None # set to zero the gradient\n",
        "  loss.backward()\n",
        "  lr = 0.01\n",
        "  W_0.data += -lr * W_0.grad\n",
        "\n",
        "print(\"train loss :\", loss.item())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Gku0J3F28zYi",
        "outputId": "64aa905e-f674-4ddf-9af4-80a75ee68b14"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "train loss : 2.309070110321045\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "xdevenc = F.one_hot(X_dev, num_classes=27).float()\n",
        "xdevenc_flat = xdevenc.view(-1, 54)\n",
        "logits = W_1[X_dev[:, 0]] + W_2[X_dev[:, 1]]\n",
        "counts = logits.exp()\n",
        "probs = counts / counts.sum(dim=1, keepdims=True)\n",
        "num = xdevenc_flat.shape[0]\n",
        "loss = -probs[torch.arange(num), y_dev].log().mean() + 0.001*(W_0**2).mean() # model smoothing with regularization\n",
        "print(\"dev loss :\", loss.item())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LegJdNv7pmVg",
        "outputId": "9e1ddf3e-f43c-4c07-a617-77d09fc705b2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "dev loss : 2.301557779312134\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "xtestenc = F.one_hot(X_test, num_classes=27).float()\n",
        "xtestenc_flat = xtestenc.view(-1, 54)\n",
        "logits = W_1[X_test[:, 0]] + W_2[X_test[:, 1]]\n",
        "counts = logits.exp()\n",
        "probs = counts / counts.sum(dim=1, keepdims=True)\n",
        "num = xtestenc_flat.shape[0]\n",
        "loss = -probs[torch.arange(num), y_test].log().mean() + 0.001*(W_0**2).mean() # model smoothing with regularization\n",
        "print(\"test loss :\", loss.item())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6af8Sfyz9UQr",
        "outputId": "1cfe8286-c650-4a86-e2c5-775f5fcd6898"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "test loss : 2.3189308643341064\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Yes, we can use indexing to replace F.one_hot"
      ],
      "metadata": {
        "id": "-c01JQXdp7ot"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### ***E05***\n",
        "look up and use F.cross_entropy instead. You should achieve the same result. Can you think of why we'd prefer to use F.cross_entropy instead?"
      ],
      "metadata": {
        "id": "k0jyOuTXPL3r"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "xtestenc = F.one_hot(X_test, num_classes=27).float()\n",
        "xtestenc_flat = xtestenc.view(-1, 54)\n",
        "logits = xtestenc_flat @ W_t\n",
        "#counts = logits.exp()\n",
        "#probs = counts / counts.sum(dim=1, keepdims=True)\n",
        "#num = xtestenc.shape[0]\n",
        "#loss = -probs[torch.arange(num), y_test].log().mean() + 0.01*(W_t**2).mean() # model smoothing with regularization\n",
        "print(\"test loss for trigram :\",F.cross_entropy(logits, y_test))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PNWDrO9ya_Ge",
        "outputId": "62e96643-a7e3-45c3-a57c-8aa8ba24a894"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "test loss for trigram : tensor(2.3216, grad_fn=<NllLossBackward0>)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "xtestenc = F.one_hot(X_test, num_classes=27).float()\n",
        "xtestenc_flat = xtestenc.view(-1, 54)\n",
        "logits = xtestenc_flat @ W_t\n",
        "counts = logits.exp()\n",
        "probs = counts / counts.sum(dim=1, keepdims=True)\n",
        "num = xtestenc.shape[0]\n",
        "loss = -probs[torch.arange(num), y_test].log().mean() + 0.01*(W_t**2).mean() # model smoothing with regularization\n",
        "print(\"test loss for trigram :\",loss.item())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "quPqIJpec67d",
        "outputId": "77b3ffcd-01bb-483f-918c-61ec8daf41e7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "test loss for trigram : 2.329163074493408\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "By using F.cross_entropy, we do not need to create counts, probs, num and loss variables, which minimize the memory use. Also, it makes computation more efficient when larger chunks of data are used"
      ],
      "metadata": {
        "id": "rdUrVFB8c-ZA"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### ***E06***\n",
        "meta-exercise! Think of a fun/interesting exercise and complete it."
      ],
      "metadata": {
        "id": "Ray9nA4fqJ_I"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# increase epochs(trigram) to 1000 and update learning rate\n",
        "for i in range(1000):\n",
        "  # forward pass\n",
        "  xenc = F.one_hot(X_train, num_classes=27).float()\n",
        "  xenc_flat = xenc.view(-1, 54)\n",
        "  logits = xenc_flat @ W_t\n",
        "  counts = logits.exp()\n",
        "  probs = counts / counts.sum(dim=1, keepdims=True)\n",
        "  num = xenc_flat.shape[0]\n",
        "  loss = -probs[torch.arange(num), y_train].log().mean() + 0.01*(W_t**2).mean() # model smoothing with regularization\n",
        "\n",
        "  # backward pass\n",
        "  W_t.grad = None # set to zero the gradient\n",
        "  loss.backward()\n",
        "\n",
        "  # update\n",
        "  if i < 750:\n",
        "    lr = 20\n",
        "  else:\n",
        "    lr = 10\n",
        "  W_t.data += -lr * W_t.grad\n",
        "print(\"train loss for trigram :\", loss.item())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QrZhT7-jqSQa",
        "outputId": "696a20cc-a81a-4f27-e876-9a0497e362b6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "train loss for trigram : 2.2536697387695312\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "xdevenc = F.one_hot(X_dev, num_classes=27).float()\n",
        "xdevenc_flat = xdevenc.view(-1, 54)\n",
        "logits = xdevenc_flat @ W_t\n",
        "counts = logits.exp()\n",
        "probs = counts / counts.sum(dim=1, keepdims=True)\n",
        "num = xdevenc.shape[0]\n",
        "loss = -probs[torch.arange(num), y_dev].log().mean() + 0.01*(W_t**2).mean()\n",
        "print(\"dev loss for trigram :\", loss.item())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "M1WwgX8ssLdw",
        "outputId": "eb5839eb-1d4e-4fa3-baea-0aa03000bb2b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "dev loss for trigram : 2.252570629119873\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "xtestenc = F.one_hot(X_test, num_classes=27).float()\n",
        "xtestenc_flat = xtestenc.view(-1, 54)\n",
        "logits = xtestenc_flat @ W_t\n",
        "counts = logits.exp()\n",
        "probs = counts / counts.sum(dim=1, keepdims=True)\n",
        "num = xtestenc_flat.shape[0]\n",
        "loss = -probs[torch.arange(num), y_test].log().mean() + 0.001*(W_t**2).mean()\n",
        "print(\"test loss for trigram :\", loss.item())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "snRDdrSusWeM",
        "outputId": "2f798193-e469-4a2b-cf61-02e691b6e481"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "test loss for trigram : 2.2577171325683594\n"
          ]
        }
      ]
    }
  ]
}