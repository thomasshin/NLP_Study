{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPOq1LRIQFg9UKxEhGjw+ZB",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/thomasshin/NLP_Study/blob/main/Transformers/Vanilla_Transformer.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import copy"
      ],
      "metadata": {
        "id": "XaL71wf3FRTl"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def clones(module, N): #produces N identical layers\n",
        "  return nn.ModuleList([copy.deepcopy(module) for _ in range(N)])"
      ],
      "metadata": {
        "id": "ZSZquP4XVQM4"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "ZKuEYyj_1LCK"
      },
      "outputs": [],
      "source": [
        "def scaled_dot_product_attention(q: torch.Tensor,\n",
        "                                 k: torch.Tensor,\n",
        "                                 v: torch.Tensor,\n",
        "                                 mask: torch.Tensor = None,\n",
        "                                 dropout: float = 0.1) -> torch.Tensor:\n",
        "  #shape of q: [B, num_heads, q_len, d_k]\n",
        "  #shape of k: [B, num_heads, k_len, d_k]\n",
        "  #shape of v: [B, num_heads, k_len, d_v]\n",
        "  k_dim = k.size()[-1]\n",
        "  attn = torch.matmul(q, k.transpose(-2,-1))\n",
        "  scale = torch.sqrt(k_dim)\n",
        "  attn_scaled = attn / scale\n",
        "\n",
        "  if mask != None: #if there is mask\n",
        "    attn_scaled = attn_scaled.masked_fill_(mask==0, torch.finfo(attn.dtype).min) #fill 0 positions with the smallest value possible\n",
        "\n",
        "  attn_weighted = torch.softmax(attn_scaled, dim=-1) #[B, num_heads, q_len, k_len]\n",
        "  attn_weighted = nn.Dropout(attn_weighted, p=dropout) #nn.dropout more convenient than F.dropout (train vs eval mode)\n",
        "  output = torch.matmul(attn_weighted, v) #[B, num_heads, q_len, d_v] (blended vector)\n",
        "\n",
        "  return output, attn_weighted"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class Attention(nn.Module):\n",
        "  def __init__(self, d_model, num_heads, dropout=0.1):\n",
        "    super().__init__()\n",
        "    assert d_model % num_heads == 0\n",
        "    self.num_heads = num_heads\n",
        "    self.head_dim = d_model // num_heads\n",
        "    self.wq = nn.Linear(d_model, d_model)\n",
        "    self.wk = nn.Linear(d_model, d_model)\n",
        "    self.wv = nn.Linear(d_model, d_model)\n",
        "    self.dropout = nn.Dropout(p=dropout)\n",
        "    self.wo = nn.Linear(d_model, d_model) #output\n",
        "\n",
        "  def split_heads(self, x, B): #current shape: [B, q_len, d_model] -> desired shape: [B, num_heads, seq_len, head_dim]\n",
        "    x = x.view(B, -1, self.num_heads, self.head_dim) #[B, seq_len, num_heads, head_dim]\n",
        "    x = x.permute(0, 2, 1, 3)\n",
        "    return x\n",
        "\n",
        "  def forward(self, q, k, v, mask=None):\n",
        "    query = self.wq(q)\n",
        "    key = self.wk(k)\n",
        "    value = self.wv(v)\n",
        "    #shape change to [B, num_heads, seq_len, head_dim]\n",
        "    B = query.size()[0]\n",
        "    qS = query.size()[1] #query seq len\n",
        "    kS = key.size()[1] #key seq len\n",
        "    query = self.split_heads(query, B)\n",
        "    key = self.split_heads(key, B)\n",
        "    value = self.split_heads(value, B)\n",
        "    scaled_attn, attn_weighted = scaled_dot_product_attention(query, key, value, mask, self.dropout) #for scaled_attn, [B, num_heads, q_len, d_v] to [B, q_len, d_model]\n",
        "    scaled_attn = scaled_attn.permute(0, 2, 1, 3) #[B, num_heads, q_len, d_v]\n",
        "    scaled_attn_concat = scaled_attn.reshape(B, qS, -1) #[B, q_len, d_model]\n",
        "    output = self.wo(scaled_attn_concat)\n",
        "\n",
        "    return output, attn_weighted #output: [B, q_len, d_model], attn_weighted: [B, num_heads, q_len, k_len]"
      ],
      "metadata": {
        "id": "31EmslozNMDz"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class TransformerEncoderLayer(nn.Module): #a single layer for Transformer encoder\n",
        "  def __init__(self, d_model, num_heads, dim_ff, dropout=0.1):\n",
        "    super().__init__()\n",
        "    self.self_attn = Attention(d_model, num_heads, dropout)\n",
        "    self.layer_norm_self_attn = nn.LayerNorm(d_model)\n",
        "    self.fc1 = nn.Linear(d_model, dim_ff)\n",
        "    self.fc2 = nn.Linear(dim_ff, d_model)\n",
        "    self.act_fn = nn.ReLU()\n",
        "    self.layer_norm_fc = nn.LayerNorm(d_model)\n",
        "    self.dropout = nn.dropout(p=dropout)\n",
        "\n",
        "  def forward(self, x, mask):\n",
        "    residual = x\n",
        "    x, attn_weighted = self.self_attn(q=x, k=x, v=x, mask=mask)\n",
        "    x = self.dropout(x)\n",
        "    x = self.layer_norm_self_attn(residual + x) #Post LN\n",
        "    residual2 = x\n",
        "    x = self.act_fn(self.fc1(x))\n",
        "    x = self.dropout(x)\n",
        "    x = self.fc2(x)\n",
        "    x = self.dropout(x)\n",
        "    x = self.layer_norm_fc(residual2 + x) #Post LN\n",
        "\n",
        "    return x, attn_weighted"
      ],
      "metadata": {
        "id": "yk0asWZyQZVA"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class TransformerEncoder(nn.Module): #a stack of encoder layers\n",
        "  def __init__(self, num_layers, src_dim, d_model, num_heads, dropout, max_seq_len = 100, dim_ff=None):\n",
        "    super().__init__()\n",
        "    self.num_layers = num_layers\n",
        "    self.tok_embed = nn.Embedding(src_dim, d_model)\n",
        "    self.pos_embed = nn.Embedding(max_seq_len, d_model)\n",
        "    self.dropout = nn.Dropout(p=0.1)\n",
        "    if dim_ff == None:\n",
        "      dim_ff = 4 * d_model #mentioned in the paper\n",
        "    single_layer = TransformerEncoderLayer(d_model, num_heads, dim_ff, dropout)\n",
        "\n",
        "    #prepare N subblocks\n",
        "    self.layers = clones(single_layer, self.num_layers)\n",
        "\n",
        "  def forward(self, x, mask=None):\n",
        "    #x: [B, src_len]\n",
        "    src_len = x.size()[1]\n",
        "    B = x.size()[0]\n",
        "    pos = torch.arange(0, src_len).unsqueeze(0).repeat(B, 1) #pos: [B, src_len]\n",
        "    x = self.dropout((self.tok_embed(x)) + self.pos_embed(pos))\n",
        "    layers_attn_scores = []\n",
        "\n",
        "    #pass the input (and the mask) through each layer in turn\n",
        "    for layer in self.layers:\n",
        "      x, attn_weighted = layer(x, mask)\n",
        "      layers_attn_scores.append(attn_weighted)\n",
        "\n",
        "    return x, layers_attn_scores"
      ],
      "metadata": {
        "id": "yJQcZU7oUum8"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class TransformerDecoderLayer(nn.Module):\n",
        "  def __init__(self, d_model, num_heads, dropout, dim_ff=None, eps=1e-12):\n",
        "    super().__init__()\n",
        "    self.masked_self_attn = Attention(d_model, num_heads, dropout)\n",
        "    self.layer_norm_self_attn = nn.LayerNorm(d_model)\n",
        "    self.cross_attn = Attention(d_model, num_heads, dropout)\n",
        "    self.layer_norm_cross_attn = nn.LayerNorm(d_model)\n",
        "    self.fc1 = nn.Linear(d_model, dim_ff)\n",
        "    self.fc2 = nn.Linear(dim_ff, d_model)\n",
        "    self.layer_norm_ff = nn.LayerNorm(d_model)\n",
        "    self.act_fn = nn.ReLU()\n",
        "    self.dropout = nn.Dropout(p=dropout)\n",
        "    if dim_ff == None:\n",
        "      dim_ff = 4 * d_model\n",
        "\n",
        "  def forward(self, x, look_ahead_mask, padding_mask, enc_output):\n",
        "    residual = x\n",
        "    x, attn_weighted_self = self.masked_self_attn(q=x, k=x, v=x, mask=look_ahead_mask)\n",
        "    x = self.dropout(x)\n",
        "    x = self.layer_norm_self_attn(residual + x)\n",
        "    residual2 = x\n",
        "    x, attn_weighted_cross = self.cross_attn(q=x, k=enc_output, v=enc_output, mask=padding_mask)\n",
        "    x = self.dropout(x)\n",
        "    x = self.layer_norm_cross_attn(residual2 + x)\n",
        "    residual3 = x\n",
        "    x = self.act_fn(self.fc1(x))\n",
        "    x = nn.dropout(x)\n",
        "    x = self.fc2(x)\n",
        "    x = nn.dropout(x)\n",
        "    x = self.layer_norm_ff(residual + x)\n",
        "\n",
        "    return x, attn_weighted_self, attn_weighted_cross\n",
        "    #x : [B, seq_len d_model]\n",
        "    #attn_weighted_self : [B, num_heads, trg_len, trg_len]\n",
        "    #attn_weighted_cross : [B, num_heads, trg_len, src_len]"
      ],
      "metadata": {
        "id": "OAjGZGSIYeiH"
      },
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class TransformerDecoder(nn.Module): #a stack of decoder layers\n",
        "  def __init__(self, num_layers, trg_dim, output_dim, d_model, num_heads, dropout, max_seq_len=100, dim_ff=None):\n",
        "    super().__init__()\n",
        "    self.num_layers = num_layers\n",
        "    self.tok_embed = nn.Embedding(trg_dim, d_model)\n",
        "    self.pos_embed = nn.Embedding(max_seq_len, d_model)\n",
        "    if dim_ff == None:\n",
        "      dim_ff = 4 * d_model\n",
        "    single_layer = TransformerDecoderLayer(d_model, num_heads, dropout, dim_ff)\n",
        "\n",
        "    #prepare N subblocks\n",
        "    self.layers = clones(single_layer, num_layers)\n",
        "\n",
        "  def forward(self, x, enc_output, look_ahead_mask=None, padding_mask=None):\n",
        "    #x : [B, tar_len, d_model]\n",
        "    #enc_output : [B, src_len, d_model]\n",
        "    #look_ahead_mask : for decoder (prevent future info)\n",
        "    #padding mask : for blending encoder's hidden states(key) with decoder's input(query), need to ignore 'pad' positioned hidden states\n",
        "    #x: [B, trg_len]\n",
        "    trg_len = x.size()[1]\n",
        "    B = x.size()[0]\n",
        "    pos = torch.arange(0, trg_len).unsqueeze(0).repeat(B, 1) #pos: [B, trg_len]\n",
        "    x = self.dropout((self.tok_embed(x)) + self.pos_embed(pos))\n",
        "    layers_attn_self = []\n",
        "    layers_attn_cross = []\n",
        "\n",
        "    #pass the input (and the mask) through each layer in turn\n",
        "    for layer in self.layers:\n",
        "      x, attn_weighted_self, attn_weighted_cross = layer(x, look_ahead_mask, padding_mask, enc_output)\n",
        "      layers_attn_self.append(attn_weighted_self)\n",
        "      layers_attn_cross.append(attn_weighted_cross)\n",
        "\n",
        "\n",
        "\n",
        "    return x, layers_attn_self, layers_attn_cross"
      ],
      "metadata": {
        "id": "RIMIQ-isU_tQ"
      },
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class Transformer(nn.Module):\n",
        "  def __init__(self, src_dim, trg_dim, num_layers, d_model, num_heads, dropout, max_seq_len=100, dim_ff=None):\n",
        "    super().__init__()\n",
        "    self.encoder = TransformerEncoder(num_layers, src_dim, d_model, num_heads, dropout, max_seq_len, dim_ff)\n",
        "    self.decoder = TransformerDecoder(num_layers, trg_dim, d_model, num_heads, dropout, max_seq_len, dim_ff)\n",
        "\n",
        "  def create_padding_mask(self, mask):\n",
        "    return mask[:, None, None, :] #[B, 1, 1, seq_len]\n",
        "\n",
        "  def create_look_ahead_mask(self, seq_len):\n",
        "    mask = torch.triu(torch.ones(seq_len, seq_len, dtype=torch.int), diagonal=1)\n",
        "    mask = 1 - mask #inverted\n",
        "    return mask\n",
        "\n",
        "  def forward(self, enc_input, dec_input, enc_pad_mask):\n",
        "    # enc_input : [B, src_len, d_model]\n",
        "    # dec_input : [B, tar_len, d_model]\n",
        "    # enc_pad_mask :\n",
        "    # - padding mask for encoder attention\n",
        "    # - padding mask for decoder's 2nd attention (to blend encoder's outputs)\n",
        "\n",
        "    #-----encoder-----\n",
        "    enc_pad_mask = self.create_padding_mask(enc_pad_mask)\n",
        "    enc_output, enc_attention = self.encoder(enc_input, enc_pad_mask)\n",
        "    #-----decoder-----\n",
        "    dec_seq_len = dec_input.size()[1]\n",
        "    look_ahead_mask = self.create_look_ahead_mask(dec_seq_len).to(dec_input.device)\n",
        "    output, dec_layer_attn_scores, dec_layer_cross_attn_scores = self.decoder(dec_input, enc_output,\n",
        "                                                                              look_ahead_mask=look_ahead_mask,\n",
        "                                                                              enc_pad_mask=enc_pad_mask)\n",
        "    return output"
      ],
      "metadata": {
        "id": "9BM5Hjv7f_EU"
      },
      "execution_count": 20,
      "outputs": []
    }
  ]
}