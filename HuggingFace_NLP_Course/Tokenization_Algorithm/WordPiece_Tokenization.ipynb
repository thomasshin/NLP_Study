{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNjK6POKeYINDg9LODizq+S",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/thomasshin/NLP_Study/blob/main/HuggingFace_NLP_Course/Tokenization_Algorithm/WordPiece_Tokenization.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YFzJa5_A1VCt",
        "outputId": "184dff2d-c741-4dda-8c7e-6f1397d293ce"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: transformers in /usr/local/lib/python3.10/dist-packages (4.33.2)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from transformers) (3.12.2)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.15.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.17.2)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (1.23.5)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from transformers) (23.1)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (6.0.1)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (2023.6.3)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from transformers) (2.31.0)\n",
            "Requirement already satisfied: tokenizers!=0.11.3,<0.14,>=0.11.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.13.3)\n",
            "Requirement already satisfied: safetensors>=0.3.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.3.3)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.10/dist-packages (from transformers) (4.66.1)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.15.1->transformers) (2023.6.0)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.15.1->transformers) (4.5.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.2.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.4)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2.0.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2023.7.22)\n"
          ]
        }
      ],
      "source": [
        "!pip install transformers"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#WordPiece tokenization\n",
        "\n",
        "WordPiece is the tokenization algorithm Google developed to pretrain BERT. It has since been reused in quite a few Transformer models based on BERT, such as DistilBERT, MobileBERT, Funnel Transformers, and MPNET. It’s very similar to BPE in terms of the training, but the actual tokenization is done differently.\n",
        "\n",
        "#Implementing WordPiece\n",
        "\n",
        "Now let’s take a look at an implementation of the WordPiece algorithm. Like with BPE, this is just pedagogical, and you won’t able to use this on a big corpus.\n",
        "\n",
        "We will use the same corpus as in the BPE example:"
      ],
      "metadata": {
        "id": "Wr-xthSh1snZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "corpus = [\n",
        "    \"This is the Hugging Face Course.\",\n",
        "    \"This chapter is about tokenization.\",\n",
        "    \"This section shows several tokenizer algorithms.\",\n",
        "    \"Hopefully, you will be able to understand how they are trained and generate tokens.\",\n",
        "]"
      ],
      "metadata": {
        "id": "CGZKjbCf7ZdP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "First, we need to pre-tokenize the corpus into words. Since we are replicating a WordPiece tokenizer (like BERT), we will use the bert-base-cased tokenizer for the pre-tokenization:"
      ],
      "metadata": {
        "id": "fJSANe017fqI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import AutoTokenizer\n",
        "\n",
        "tokenizer_bert = AutoTokenizer.from_pretrained(\"bert-base-cased\")"
      ],
      "metadata": {
        "id": "LRdtEcCg7dHG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Then we compute the frequencies of each word in the corpus as we do the pre-tokenization:"
      ],
      "metadata": {
        "id": "rA3--hvb7sLT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from collections import defaultdict\n",
        "\n",
        "word_freqs = defaultdict(int)\n",
        "for text in corpus:\n",
        "  words_with_offsets = tokenizer_bert.backend_tokenizer.pre_tokenizer.pre_tokenize_str(text)\n",
        "  new_words = [word for word, offset in words_with_offsets]\n",
        "  for word in new_words:\n",
        "    word_freqs[word] += 1\n",
        "\n",
        "word_freqs"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QBS7OIQs7rOS",
        "outputId": "9b7359be-f87a-40d6-8d61-e8fcadb1ae2b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "defaultdict(int,\n",
              "            {'This': 3,\n",
              "             'is': 2,\n",
              "             'the': 1,\n",
              "             'Hugging': 1,\n",
              "             'Face': 1,\n",
              "             'Course': 1,\n",
              "             '.': 4,\n",
              "             'chapter': 1,\n",
              "             'about': 1,\n",
              "             'tokenization': 1,\n",
              "             'section': 1,\n",
              "             'shows': 1,\n",
              "             'several': 1,\n",
              "             'tokenizer': 1,\n",
              "             'algorithms': 1,\n",
              "             'Hopefully': 1,\n",
              "             ',': 1,\n",
              "             'you': 1,\n",
              "             'will': 1,\n",
              "             'be': 1,\n",
              "             'able': 1,\n",
              "             'to': 1,\n",
              "             'understand': 1,\n",
              "             'how': 1,\n",
              "             'they': 1,\n",
              "             'are': 1,\n",
              "             'trained': 1,\n",
              "             'and': 1,\n",
              "             'generate': 1,\n",
              "             'tokens': 1})"
            ]
          },
          "metadata": {},
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "word_freqs"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nb8xrk2W8wnk",
        "outputId": "1672ee9d-f963-4a64-e319-c6196d77d5fe"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "defaultdict(int,\n",
              "            {'This': 3,\n",
              "             'is': 2,\n",
              "             'the': 1,\n",
              "             'Hugging': 1,\n",
              "             'Face': 1,\n",
              "             'Course': 1,\n",
              "             '.': 4,\n",
              "             'chapter': 1,\n",
              "             'about': 1,\n",
              "             'tokenization': 1,\n",
              "             'section': 1,\n",
              "             'shows': 1,\n",
              "             'several': 1,\n",
              "             'tokenizer': 1,\n",
              "             'algorithms': 1,\n",
              "             'Hopefully': 1,\n",
              "             ',': 1,\n",
              "             'you': 1,\n",
              "             'will': 1,\n",
              "             'be': 1,\n",
              "             'able': 1,\n",
              "             'to': 1,\n",
              "             'understand': 1,\n",
              "             'how': 1,\n",
              "             'they': 1,\n",
              "             'are': 1,\n",
              "             'trained': 1,\n",
              "             'and': 1,\n",
              "             'generate': 1,\n",
              "             'tokens': 1})"
            ]
          },
          "metadata": {},
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "As we saw before, the alphabet is the unique set composed of all the first letters of words, and all the other letters that appear in words prefixed by ##:"
      ],
      "metadata": {
        "id": "LU3mVh9D8c9a"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "alphabet = []\n",
        "\n",
        "for word in word_freqs.keys():\n",
        "  if word[0] not in alphabet:\n",
        "    alphabet.append(word[0])\n",
        "  for letter in word[1:]:\n",
        "    if f\"##{letter}\" not in alphabet:\n",
        "      alphabet.append(f\"##{letter}\")\n",
        "\n",
        "alphabet.sort()\n",
        "alphabet"
      ],
      "metadata": {
        "id": "CFKYbjwZ8dRw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We also add the special tokens used by the model at the beginning of that vocabulary. In the case of BERT, it’s the list [\"[PAD]\", \"[UNK]\", \"[CLS]\", \"[SEP]\", \"[MASK]\"]:"
      ],
      "metadata": {
        "id": "P_bkfQNR_LRR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "vocabs =  [\"[PAD]\", \"[UNK]\", \"[CLS]\", \"[SEP]\", \"[MASK]\"] + alphabet.copy()"
      ],
      "metadata": {
        "id": "gjN32MeO_LcP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Next we need to split each word, with all the letters that are not the first prefixed by ##:"
      ],
      "metadata": {
        "id": "FGRn_4JU_STW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "splits = {word : [c if i ==0 else f\"##{c}\" for i, c in enumerate(word)] for word in word_freqs.keys()}\n",
        "splits"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rc1m8k8O_SmT",
        "outputId": "03e005e6-070a-445b-d2ce-00784f868ca5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'This': ['T', '##h', '##i', '##s'],\n",
              " 'is': ['i', '##s'],\n",
              " 'the': ['t', '##h', '##e'],\n",
              " 'Hugging': ['H', '##u', '##g', '##g', '##i', '##n', '##g'],\n",
              " 'Face': ['F', '##a', '##c', '##e'],\n",
              " 'Course': ['C', '##o', '##u', '##r', '##s', '##e'],\n",
              " '.': ['.'],\n",
              " 'chapter': ['c', '##h', '##a', '##p', '##t', '##e', '##r'],\n",
              " 'about': ['a', '##b', '##o', '##u', '##t'],\n",
              " 'tokenization': ['t',\n",
              "  '##o',\n",
              "  '##k',\n",
              "  '##e',\n",
              "  '##n',\n",
              "  '##i',\n",
              "  '##z',\n",
              "  '##a',\n",
              "  '##t',\n",
              "  '##i',\n",
              "  '##o',\n",
              "  '##n'],\n",
              " 'section': ['s', '##e', '##c', '##t', '##i', '##o', '##n'],\n",
              " 'shows': ['s', '##h', '##o', '##w', '##s'],\n",
              " 'several': ['s', '##e', '##v', '##e', '##r', '##a', '##l'],\n",
              " 'tokenizer': ['t', '##o', '##k', '##e', '##n', '##i', '##z', '##e', '##r'],\n",
              " 'algorithms': ['a',\n",
              "  '##l',\n",
              "  '##g',\n",
              "  '##o',\n",
              "  '##r',\n",
              "  '##i',\n",
              "  '##t',\n",
              "  '##h',\n",
              "  '##m',\n",
              "  '##s'],\n",
              " 'Hopefully': ['H', '##o', '##p', '##e', '##f', '##u', '##l', '##l', '##y'],\n",
              " ',': [','],\n",
              " 'you': ['y', '##o', '##u'],\n",
              " 'will': ['w', '##i', '##l', '##l'],\n",
              " 'be': ['b', '##e'],\n",
              " 'able': ['a', '##b', '##l', '##e'],\n",
              " 'to': ['t', '##o'],\n",
              " 'understand': ['u',\n",
              "  '##n',\n",
              "  '##d',\n",
              "  '##e',\n",
              "  '##r',\n",
              "  '##s',\n",
              "  '##t',\n",
              "  '##a',\n",
              "  '##n',\n",
              "  '##d'],\n",
              " 'how': ['h', '##o', '##w'],\n",
              " 'they': ['t', '##h', '##e', '##y'],\n",
              " 'are': ['a', '##r', '##e'],\n",
              " 'trained': ['t', '##r', '##a', '##i', '##n', '##e', '##d'],\n",
              " 'and': ['a', '##n', '##d'],\n",
              " 'generate': ['g', '##e', '##n', '##e', '##r', '##a', '##t', '##e'],\n",
              " 'tokens': ['t', '##o', '##k', '##e', '##n', '##s']}"
            ]
          },
          "metadata": {},
          "execution_count": 48
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now that we are ready for training, let’s write a function that computes the score of each pair. We’ll need to use this at each step of the training:"
      ],
      "metadata": {
        "id": "Id49G2OXAIKj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def compute_pair_scores(splits):\n",
        "  letter_freqs = defaultdict(int)\n",
        "  pair_freqs = defaultdict(int)\n",
        "  for word, freq in word_freqs.items():\n",
        "    split = splits[word]\n",
        "    if len(split) == 1:\n",
        "      letter_freqs[split[0]] += freq\n",
        "      continue\n",
        "    for i in range(len(split)-1):\n",
        "      pair = (split[i], split[i+1])\n",
        "      pair_freqs[pair] += freq\n",
        "      letter_freqs[split[i]] += freq\n",
        "    letter_freqs[split[-1]] += freq\n",
        "\n",
        "  scores = {\n",
        "      pair : freq / (letter_freqs[pair[0]] * letter_freqs[pair[1]]) for pair, freq in pair_freqs.items()\n",
        "  }\n",
        "\n",
        "  return scores"
      ],
      "metadata": {
        "id": "LgQKc2AYkWjd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pair_scores = compute_pair_scores(splits)\n",
        "for i, key in enumerate(pair_scores.keys()):\n",
        "    print(f\"{key}: {pair_scores[key]}\")\n",
        "    if i >= 5:\n",
        "        break"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "k6YxywXD4G-t",
        "outputId": "ca2f0ba1-ba18-4722-ae47-2515e8bae155"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "('T', '##h'): 0.125\n",
            "('##h', '##i'): 0.03409090909090909\n",
            "('##i', '##s'): 0.02727272727272727\n",
            "('i', '##s'): 0.1\n",
            "('t', '##h'): 0.03571428571428571\n",
            "('##h', '##e'): 0.011904761904761904\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "best_pair = \"\"\n",
        "max_score = None\n",
        "\n",
        "for pair, score in pair_scores.items():\n",
        "  if max_score == None or score > max_score:\n",
        "    max_score = score\n",
        "    best_pair = pair\n",
        "best_pair, max_score"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "L_1RW5b-5Aaa",
        "outputId": "f1ad6075-a51d-48b6-e8bc-70f8941261c3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(('a', '##b'), 0.2)"
            ]
          },
          "metadata": {},
          "execution_count": 21
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "So the first merge to learn is ('a', '##b') -> 'ab', and we add 'ab' to the vocabulary:"
      ],
      "metadata": {
        "id": "RE4JjBn45WGK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "vocabs.append(\"ab\")"
      ],
      "metadata": {
        "id": "DBnkBvFK5WcB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "merge_pair(\"a\", \"b\", spl"
      ],
      "metadata": {
        "id": "B3oZAgqm73M6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def merge_pair(a, b, splits):\n",
        "  for word in word_freqs:\n",
        "    split = splits[word]\n",
        "    if len(split) == 1:\n",
        "      continue\n",
        "    i = 0\n",
        "    while i < len(split)-1:\n",
        "      if split[i] == a and split[i+1] == b:\n",
        "        merge = a + b[2:] if b.startswith(\"##\") else a + b\n",
        "        split = split[:i] + [merge] + split[i+2:]\n",
        "      else:\n",
        "        i += 1\n",
        "    splits[word] = split\n",
        "  return splits"
      ],
      "metadata": {
        "id": "E8M5fEu75ZOI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def merge_pair(a, b, splits):\n",
        "    for word in word_freqs:\n",
        "        split = splits[word]\n",
        "        if len(split) == 1:\n",
        "            continue\n",
        "        i = 0\n",
        "        while i < len(split) - 1:\n",
        "            if split[i] == a and split[i + 1] == b:\n",
        "                merge = a + b[2:] if b.startswith(\"##\") else a + b\n",
        "                split = split[:i] + [merge] + split[i + 2 :]\n",
        "            else:\n",
        "                i += 1\n",
        "        splits[word] = split\n",
        "    return splits"
      ],
      "metadata": {
        "id": "ijH1uc_JAsyX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "splits = merge_pair(\"a\", \"##b\", splits)\n",
        "splits[\"about\"]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LjSPQbFXAUla",
        "outputId": "2c453845-17ce-4d2e-e349-bc35ef1f31e5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['ab', '##o', '##u', '##t']"
            ]
          },
          "metadata": {},
          "execution_count": 50
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now we have everything we need to loop until we have learned all the merges we want. Let’s aim for a vocab size of 70:"
      ],
      "metadata": {
        "id": "o3Mfbsi_DXwF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "vocab_size = 70\n",
        "\n",
        "while len(vocabs) < vocab_size:\n",
        "  scores = compute_pair_scores(splits)\n",
        "  best_pair, max_score = \"\", None\n",
        "  for pair, score in scores.items():\n",
        "    if max_score == None or max_score < score:\n",
        "      max_score = score\n",
        "      best_pair = pair\n",
        "  splits = merge_pair(*best_pair, splits)\n",
        "  new_token = (\n",
        "      best_pair[0] + best_pair[1][2:] if best_pair[1].startswith(\"##\") else best_pair[0] + best_pair[1]\n",
        "  )\n",
        "  vocabs.append(new_token)\n",
        "\n",
        "vocabs"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Fhj4Q6ggDWOZ",
        "outputId": "715728fb-eb86-4d17-f1f6-473580ae43b2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['[PAD]',\n",
              " '[UNK]',\n",
              " '[CLS]',\n",
              " '[SEP]',\n",
              " '[MASK]',\n",
              " '##a',\n",
              " '##b',\n",
              " '##c',\n",
              " '##d',\n",
              " '##e',\n",
              " '##f',\n",
              " '##g',\n",
              " '##h',\n",
              " '##i',\n",
              " '##k',\n",
              " '##l',\n",
              " '##m',\n",
              " '##n',\n",
              " '##o',\n",
              " '##p',\n",
              " '##r',\n",
              " '##s',\n",
              " '##t',\n",
              " '##u',\n",
              " '##v',\n",
              " '##w',\n",
              " '##y',\n",
              " '##z',\n",
              " ',',\n",
              " '.',\n",
              " 'C',\n",
              " 'F',\n",
              " 'H',\n",
              " 'T',\n",
              " 'a',\n",
              " 'b',\n",
              " 'c',\n",
              " 'g',\n",
              " 'h',\n",
              " 'i',\n",
              " 's',\n",
              " 't',\n",
              " 'u',\n",
              " 'w',\n",
              " 'y',\n",
              " 'ab',\n",
              " 'Fa',\n",
              " 'Fac',\n",
              " '##ct',\n",
              " '##ful',\n",
              " '##full',\n",
              " '##fully',\n",
              " 'Th',\n",
              " 'ch',\n",
              " '##hm',\n",
              " 'cha',\n",
              " 'chap',\n",
              " 'chapt',\n",
              " '##thm',\n",
              " 'Hu',\n",
              " 'Hug',\n",
              " 'Hugg',\n",
              " 'sh',\n",
              " 'th',\n",
              " 'is',\n",
              " '##thms',\n",
              " '##za',\n",
              " '##zat',\n",
              " '##ut',\n",
              " '##ta']"
            ]
          },
          "metadata": {},
          "execution_count": 52
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "As we can see, compared to BPE, this tokenizer learns parts of words as tokens a bit faster.\n",
        "\n",
        "To tokenize a new text, we pre-tokenize it, split it, then apply the tokenization algorithm on each word. That is, we look for the biggest subword starting at the beginning of the first word and split it, then we repeat the process on the second part, and so on for the rest of that word and the following words in the text:"
      ],
      "metadata": {
        "id": "KYB5FxFCFV1x"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def encode_word(word):\n",
        "  tokens = []\n",
        "  while len(word) > 0:\n",
        "    i = len(word)\n",
        "    while i > 0 and word[:i] not in vocabs:\n",
        "      i -= 1\n",
        "    if i == 0:\n",
        "      return [[\"UNK\"]]\n",
        "    tokens.append(word[:i])\n",
        "    word = word[i:]\n",
        "    if len(word) > 0:\n",
        "      word = f\"##{word}\"\n",
        "  return tokens"
      ],
      "metadata": {
        "id": "p1fk16dGFWP5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(encode_word(\"Hugging\"))\n",
        "print(encode_word(\"HOgging\"))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "69XMFjGALwMR",
        "outputId": "fd97c880-1938-4427-f728-8bb46183a56c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['Hugg', '##i', '##n', '##g']\n",
            "[['UNK']]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now, let’s write a function that tokenizes a text:"
      ],
      "metadata": {
        "id": "j0Yp13a8LyRC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def tokenize(text):\n",
        "  pre_tokenized_result = tokenizer_bert.backend_tokenizer.pre_tokenizer.pre_tokenize_str(text)\n",
        "  pre_tokenized_text = [word for word, offset in pre_tokenized_result]\n",
        "  encoded_words = [encode_word(word) for word in pre_tokenized_text]\n",
        "  return sum(encoded_words, [])"
      ],
      "metadata": {
        "id": "t9uBuTSeLw-f"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tokenize(\"This is the Hugging Face course!\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wgrM09ZQNDGL",
        "outputId": "f824678f-bb00-43ab-e03d-bf5f7e390a9b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['Th',\n",
              " '##i',\n",
              " '##s',\n",
              " 'is',\n",
              " 'th',\n",
              " '##e',\n",
              " 'Hugg',\n",
              " '##i',\n",
              " '##n',\n",
              " '##g',\n",
              " 'Fac',\n",
              " '##e',\n",
              " 'c',\n",
              " '##o',\n",
              " '##u',\n",
              " '##r',\n",
              " '##s',\n",
              " '##e',\n",
              " ['UNK']]"
            ]
          },
          "metadata": {},
          "execution_count": 56
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "That’s it for the WordPiece algorithm! Now let’s take a look at Unigram."
      ],
      "metadata": {
        "id": "ybAuMX5_NGr3"
      }
    }
  ]
}